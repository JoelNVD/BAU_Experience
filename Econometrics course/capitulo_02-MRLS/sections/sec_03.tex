%===============================================================================
\section{Propiedades y supuestos del MCO}
%===============================================================================

%-------------------------------------------------------------------------------
\subsection{Propiedades del MCO}
%-------------------------------------------------------------------------------
\begin{frame}{Propiedades algebraicas}
	\begin{itemize}
		\item $\sum_{i=1}^{n}\hat{u_{i}}=0 \Leftrightarrow \frac{\sum_{i=1}^{n}\hat{u_{i}}}{n}=0$
		\item $\sum_{i=1}^{n}x_{i}\hat{u_{i}}=0$
		\item $\overline{y}=\hat{\beta_{o}}+\hat{\beta_{1}}\overline{x}$
	\end{itemize}
\end{frame}
%------------------------------------------------
\begin{frame}{Suma Cuadrada}
	Cada observación se puede descomponer de una parte explicada y otra no explicada: $y{i}=\hat{y_{i}}+\hat{u_{i}}$. A partir de esto definimos:
		\begin{description}
			\item[STC] Suma total de cuadrados $\sum_{i=1}^{n}(y_{i}-\overline{y})^{2}$
			\item[SEC] Suma explicada al cuadrado $\sum_{i=1}^{n}(\hat{y_{i}}-\overline{y})^{2}$
			\item[SRC] Suma de residuos al cuadrado $\sum_{i=1}^{n}(\hat{u_{i}})^{2}$
			\item[Finalmente] \textcolor{red}{STC=SEC+SRC} Probar!
			\item[$R^{2}$] Es el indicador de ajuste más popular empleado para medir que tan bien el modelo se ajusta a los datos:\\
			$R^{2}=\frac{SEC}{STC}=1-\frac{SRC}{STC}$
		\end{description}
\end{frame}

%-------------------------------------------------------------------------------
\subsection{Más supuestos del MCO}
%-------------------------------------------------------------------------------
\begin{frame}{Supuestos 1-4}
	\begin{enumerate}
		\item Linealidad de los parámetros
		$$y=\beta_{0}+\beta_{1}x+\mu$$
		\item Muestreo aleatorio
		\item Variación muestral de la variable explicativa
		\item Media condicional cero del error:
		$$E(\mu / x) = E(\mu) = 0$$
	\end{enumerate}
\end{frame}
%------------------------------------------------
\begin{frame}{Supuestos 1-4}
	Con los supuestos 1 a 4 se prueba que los estimadores son insesgados:
	\begin{align*}
		E(\hat{\beta}_{1}) &= \beta_{1}\\
		E(\hat{\beta}_{0}) &= \beta_{0}
	\end{align*}
\end{frame}
%------------------------------------------------
\begin{frame}{Estimadores insesgados}
		\begin{align*}
			\widehat{\beta}_1 &= \frac{\sum (x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum (x_{i}-\overline{x})^2}\\
			& = \frac{\sum (x_{i}-\overline{x})\{\beta_1(x_{i}-\overline{x})+u_i\}}{\sum (x_{i}-\overline{x})^2}\\
			& = \beta_1\frac{\sum (x_{i}-\overline{x})^2}{\sum (x_{i}-\overline{x})^2} + \frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2}\\
			& = \beta_1 + \frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2} \tag{2}
		\end{align*}
	Tomando el esperado y asumiendo que $X$ no es estocástico (¡por cierto, una suposición fuerte!)
		\begin{align*}
			E[\widehat{\beta}_1] & = \beta_1 + E\left[\frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2} \right]
		\end{align*}
\end{frame}
%------------------------------------------------
\begin{frame}{Estimadores insesgados}
	Finalmente
		\begin{align*}
			E[\widehat{\beta}_1] & = \beta_1 + E\left[\frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2}\right]\\
								 & = \beta_1 + \frac{\sum (x_{i}-\overline{x})E[u_i]}{\sum (x_{i}-\overline{x})^2}\\
								 & = \beta_1
		\end{align*}
	Hemos demostrado que el estimador MCO es un estimador insesgado. Pero, todavía necesitamos la variación.
\end{frame}
%------------------------------------------------
\begin{frame}{Supuestos 5: Homocedasticidad}
	Supuesto 5:
	$$Var(\mu / x) = \sigma^{2}$$
	Sabiendo que:
	$$\hat{\beta}_{1} = \beta_{1} + \frac{\sum (x_{i}-\overline{x})\mu_{i}}{\sum (x_{i}-\overline{x})^2}$$
	Se demuestra que:
	$$Var(\hat{\beta}_{1})=\frac{\sigma^{2}}{\sum (x_{i}-\overline{x})^2}$$
	$\sigma^{2}$ no es conocido, pero puede ser estimado a partir de los residuales $\epsilon$.
\end{frame}
\begin{frame}{Estimadores eficientes}
	Veamos detenidamente la expresión (2)
		$$\widehat{\beta}_1 = \beta_1 + \frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2} \quad \rightarrow \quad \widehat{\beta}_1 - \beta_1 = \frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2} $$
	llevando cuadrados a ambas expresiones
		$$\left[ \widehat{\beta}_1 - \beta_1\right]^2 = \left[\frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2}\right]^2 $$
	tomando el valor esperado
		$$E\left[ \widehat{\beta}_1 - \beta_1\right]^2 = E\left[\frac{\sum (x_{i}-\overline{x})u_i}{\sum (x_{i}-\overline{x})^2}\right]^2 = \frac{\sum (x_{i}-\overline{x})^2E(u_i^2)}{\left[\sum (x_{i}-\overline{x})^2\right]^2}$$
	\textit{Nota:} $E\left[ \widehat{\beta}_1 - \beta_1\right]^2 \equiv \widehat{\sigma}_{\widehat{\beta}_1}^{2}$
\end{frame}
\begin{frame}{Estimadores eficientes}
	reescribiendo la expresión anterior y teniendo en cuenta la característica i.i.d de u
		\begin{align*}
			E\left[ \widehat{\beta}_1 - \beta_1\right]^2 & = \frac{\sum (x_{i}-\overline{x})^2E(u_i^2)}{\left[\sum (x_{i}-\overline{x})^2\right]^2}\\
			& = \frac{\sum (x_{i}-\overline{x})^2E[u_i^2]}{\left[\sum (x_{i}-\overline{x})^2\right]^2}\\
			& = E[u_i^2] \frac{\sum (x_{i}-\overline{x})^2}{\left[\sum (x_{i}-\overline{x})^2\right]^2} \tag{3}
		\end{align*}
	finalmente
		$$E\left[ \widehat{\beta}_1 - \beta_1\right]^2 = \sigma^{2}\frac{1}{\sum (x_{i}-\overline{x})^2}$$
	donde $E[u^2]=\sigma^{2} \equiv s_{u}^{2}$. Estamos asumiendo homocedasticidad.
\end{frame}
\begin{frame}{Estimadores eficientes}
	¡Casi ahí!. Solo lo que necesitamos saber es el error estándar del error (u). Sabemos que la desviación estándar de la estimación MCO -en el caso de un regresor- es
		\begin{align*}
			Var(\widehat{\beta}_1) = \sigma^{2}\frac{1}{\sum (x_{i}-\overline{x})^2} \tag{4}
		\end{align*}
	¿Qué es $\sigma^{2}$?. Necesitamos un estimador (insesgado) para $\sigma^{2}$. Entonces, tenemos la siguiente propuesta:
		$$\widehat{\sigma}^{2} = \frac{\sum u^2}{(n-2)}$$
	Entonces:
		$$Var(\widehat{\beta}_1) = \widehat{\sigma}^{2}\frac{1}{\sum (x_{i}-\overline{x})^2}$$
\end{frame}
%------------------------------------------------
\begin{frame}{Estimador de $\sigma^{2}$}
	\begin{align}
		\epsilon	& = y - \hat{y}\\
		& = (\beta_{o}+\beta_{1}x+\mu) - (\hat{\beta}_{0}+\hat{\beta}_{1}x)\\
		\epsilon	& = (\beta_{0} - \hat{\beta}_{0}) - (\hat{\beta}_{1} - \beta_{1}) + \mu\\
		\overline{\epsilon}	& = (\beta_{0} - \hat{\beta}_{0}) - (\hat{\beta}_{1} - \beta_{1}) + \overline{\mu},\enskip \textcolor{red}{promedio\enskip (3)}\\
		\epsilon - \overline{\epsilon}	& = (\mu - \overline{\mu}) - (\hat{\beta}_{1} - \beta_{1})(x - \overline{x}),\enskip \textcolor{red}{(3-4)} \\
		\epsilon	& = (\mu - \overline{\mu})- (\hat{\beta}_{1} - \beta_{1})(x - \overline{x}),\enskip \textcolor{red}{Ec. Normal}
	\end{align}
\end{frame}
%------------------------------------------------
\begin{frame}{Estimador de $\sigma^{2}$}
	Finalmente:
	$$\epsilon^{2} = (\mu - \overline{\mu})^{2} + (\hat{\beta}_{1} - \beta_{1})^{2}(x - \overline{x})^{2}-2(\mu - \overline{\mu})(\hat{\beta}_{1} - \beta_{1})(x - \overline{x})$$
	Aplicando sumatoria:
	\begin{align*}
		\sum\epsilon^2	& = \sum(\mu-\bar\mu)^2 \\
		&  +(\hat\beta_1 -\beta_1)^2\sum(x-\bar x)^2\\
		&  -2 (\mu-\bar\mu)(\hat\beta_1 -\beta_1)\sum(x-\bar x)
	\end{align*}
	Esperanza matemática:
	\begin{align*}
		E\left[\sum\epsilon^2\right]	&=\textcolor{red}{\sum E(\mu-\bar\mu)^2} \\
		&+ \textcolor{blue}{E(\hat\beta_1 -\beta_1)^2\sum(x-\bar x)^2}\\
		&-\textcolor{green}{2 E(\mu-\bar\mu)(\hat\beta_1 -\beta_1)\sum(x-\bar x)} 
	\end{align*}
\end{frame}
%------------------------------------------------
\begin{frame}{Estimador de $\sigma^{2}$}
	Se puede probar que:
	\begin{align*}
		\textcolor{red}{\sum E (\mu-\bar\mu)^2} & = (n-1)\sigma^2 \\
		\textcolor{blue}{E(\hat\beta_1 -\beta_1)^2\sum(x-\bar x)^2} & = \sigma^2 \\
		\textcolor{green}{2 E(\mu-\bar\mu)(\hat\beta_1 -\beta_1)\sum(x-\bar x)} & =  2\sigma^2 
	\end{align*}
	Finalmente:
	\begin{align*}
		E\left[\sum\epsilon^2\right] &= (n-1)\sigma^2 +  \sigma^2 -2\sigma^2  \\
		E\left[\sum\epsilon^2/(n-2) \right] &= \sigma^2 
	\end{align*}
	$\hat\sigma^2=\sum\epsilon^2/(n-2)$ es un estimador insesgado de $\sigma^2$.
\end{frame}
%------------------------------------------------
\begin{frame}{Supuesto 6 para la inferencia}
	\begin{itemize}
		\item \textcolor{blue}{Supuesto 6: $\mu \thicksim N(0,\sigma^2)$}
		\item Implica que 
		$$y/x \thicksim N(\beta_{0}+\beta_{1}x,\sigma^2)$$
		\item También 
		$$\hat{\beta_{1}}=N\left(\beta_{1},\frac{\sigma^2}{\sum (x-\bar x)^2}\right)$$ 
		\item Por lo que estandarizando se tiene:
		$$\frac{\hat{\beta_{1}}-\beta_{1}}{\sigma/\sqrt{\sum (x-\bar x)^2}} \thicksim N(0,1)$$
	\end{itemize}
\end{frame}
